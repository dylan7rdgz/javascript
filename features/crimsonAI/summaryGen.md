# View this Documenation on a code editor or via chatGPT.

# GPT Powered Section-wise Summaries Technical Documentation

## Timelines

1. **Requirement Gathering and POC** \[1.5 days\]
2. **Development** \[5.5 days\]
      
   1. Copy Existing Summary if already generated
      1. Unit Tests. [2hrs]
      1. !!! Use an account, literature input data which can help in testing effectively  
      2. (Write + Test) SQL Queries to select and copy columns of existing literature [2hrs]
      3. Integrate this query in the handlers/helpers [2hrs]
      4. select and copy columns from other tables containing information/metadata relevant to the literature 
   2. Find the correct Chat GPT API
      1. Find chat GPT API Online [1.5hrs]
      2. Check ways in which we can use chatGPT effectively without spending on tokens
      2. Test with the chosen chat GPT API Independently via postman [1hrs]
   3. Integrate Chat GPT API in the application
      1. Remove/Comment/Clean code* that listens to the key-insights-response queue [5hrs]
      2. Preproccessing Data before sending ito to GPT [3hrs]
      3. Create/Test a wrapper over Chap GPI API [1hr] 
      4. Processing Data [3hrs]
          Challenges Anticipated: A lot of tediousness in processing JSON objects ~ Think of effective ways of processing them.
         !! processing code written independently in js, had to manually complile to ts
         !! Code was written using Promise.allSetlle which is not supported by the ts compiler the serverless code uses
         1. RAXTER-KI Server has its own format for storing summaries; understand the existing structure [2hrs] 
         2. Each section of summary has its own structure including generated UUID strings
         !!! 1.1. Withing Each Paragraph certain special symbols need to be handled like "" within the strings, etc
         3. Check the system to see if we generate a random string or no string will there be any impact in the system. [3hrs]
         4. Finalize the new structure and make sure it doesn't violate any contract
         5. Refactoring might be required in the GET call to fetch summary if in case 4. is inevitable [4.5hrs]
      5. Storing processed data (literature summaries) in DB 
      6. send metadata to AdvMinerQueue [1hr]

3. **E2E Dev Testing** [1.5hrs]
   1. If a literature does not have a DOI, then the Summary should be generated by chatGPT
   2. No summary is generated if the same literature is submittied again. The old summary also does not get displayed. Instead, a button is displayed asking user to generate a summary.
   3. If the user exists summary modal "t.selectedPlan is undefined" error message is thrown


   n. Error Handling
      1. 
      2. Unable to post to connection [not a side effect of chatGPTSum gen]
         -> PayloadTooLargeException [statusCode: 413, retryable: false, retryDelay: 91.914]


   ~ Error Message types:
   Payload too large ():
      https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logsV2:log-groups/log-group/$252Faws$252Flambda$252Fliteratures-parse-extract-beta-literature-parse-response-handler/log-events/2023$252F08$252F29$252F$255B$2524LATEST$255De8abfee2bea4443690b4340851f0a59b
   Unable to post to connection [ occurs at send-socket-event]
   


4. **QA Testing** [Will be shared by Bhushan]
5. **Follow-up with QA** \[0.5 days\]
6. **Release** \[0.5 days\]

## Technical Challenges

1. **No clarity on which Chat GPT summary to use**
   - Does the DS team have any idea or a wrapper over OPEN API for generating "section-wise summary"?
   - cc @Apurva Nagvenkar
   - Does the Product team have a specific OPEN AI API in mind for use?
   - cc @Harsh Chudasama
   - Am I expected to search online for an OPEN AI API to generate section-wise summaries?

## Approach

1. **Find an Open AI API for Summaries**
   1. If an API only accepts "plain text" for generating summaries:
      - Multiple requests are needed to be sent for each PDF section received from the PDF parser server.
      - Queues might need to be used to stabilize requests.
      - Ensure sections from different users' literatures do not interleave. [Consider using checksums for separation or delimiting the summary sections of complete literatures]

   2. If an API accepts offsets along with plain text: **This is an easier approach**
      - Send start and end indexes of sections provided by the PDF parser server. In this case ChatGPT will take care of the sections since we are providing the indexes.

2. **Flow to Get the DOI from <PDF parser server>** [Part A]
   - <Raxter-Api server> (Node.js) and <serverless>(Node.js) lack DOI extraction and text extraction (from a literature) capabilities.
   - The Responsibility lies with the <PDF parser server> (Python) for extracting DOI and text.

3. **Flow Once we get the DOI from PDF parser server: Old vs. New** [Part B]
   - **Old Flow** [This flow needs to be removed]:
     - Receive parsed data from <PDF parser server>"
     - Send literatureId to <KI server> via <KI-request-Queue>.
     - KI server generates "summary sections" and dumps them to <KI-response-Queue.>
     - once we read the data from queue the <serverless> system updates MySQL.

   - **New Flow** [This flow needs to be encorporated]:
     - Receive parsed data from pdf-parsed-response-queue.
     - Make a request to the Chat GPT summary API using methods described in "1."

   - *IMPORTANT!* Let the KI-Server Generate and store the results in DB.
     - We need to create new cols to store Chat GPT summary
     - WHY?? In the API in which we get the summary, we can use exsiting meta-info from EXISTING_SUMMARIES to help us generate NEW_CHAT_GPT_SUMMARIES while maintaining its structure. [Also, there will be no DB I/O overhead since these NEW_CHAT_GPT_SUMMARIES are in the same col.]
     - ~Challenge Anticipated: EXISTING_SUMMARIES and NEW_CHAT_GPT_SUMMARIES should be render-able 
       i.e if code is refactored to use sentences generated by chat_gpt the code should not break while
       generating the structure of old summary sentences

## Optimizations/Considerations

- **Streaming Usage**
  - Explore the possibility of using streaming for improved performance.

- **Non-Blocking and Asynchronous Code**
  - Refactor code for non-blocking and high asynchronicity.

- **Zip/Unzip Payloads**
  - Consider zipping/unzipping request payloads for large transmission data.


## Mistakes & Issues [Nothing is more valuable than experience]
   - AWS cloudwatch logs continue to get logged in a particular log GROUP. Do not expect a new GROUP to be created with a new timestamp everytime a new request is sent 
   Challenges faced:
   1. Summary was getting overwritten with old summary. Debugged this on raxter backend, serverless. There was no insert query or update query that was doing this. Then I got on call with Shubham. AI server also was not doing this update. Then after debugging, I got  to know that there was a race condition in the application. 
   We have one Queue and two lambdas (beta, qat). Sometimes beta [having old code to generate the summary] would pick up the summary and store in DB
   2. Also, this was causing other confusions like why the logs are not getting pushed to cloudwatch. Because of this, it was difficult to test the code for summary gen. end to end.
   In serverless there is no way to test this e2e so I have to rely on cloudwatch logs to make changes. 
   I could use jest to test, but I had to repetitively make changes in code and tests to make the tests run successfully.
   3. Biased mind - I wrote an wrong insert query and believed it was right. I thought that summaries are stored in literatures table and not literatures summary table and continued to believe this was true.
   4. Implementing "copying existing literatures conditionally" was quite ticky (the if part). I couldn't understand why two records were getting created. The Reason - One of them was getting created while uploading the literature and the other unnecssary as well as eroneous helper to copyAnExstingLiterature was not needed. I then tried to use updateAnExistingLiterature.
   As soon as I did this I realised that this also was not needed because summaries are strores in literature_summaries and not literature table.
   5. Always use a setup and teardown while writing unit tests. 
   6.  
   7. I could not understand why google chrome was not opening. Later on I realized that the HDMI cable was plugged into my computer and the chrome app was open on the screen. I could not see this on the screen because it was in sleep mode.



## Analysis of chatGPT request fails:

literature id                                                                                       
106622            huge paragraphs in a batch of 4     4/4 successful     12424.13ms

   

## Bugs:
1. If the Frontend is throwing an error it should have a proper callstack
2. Vuex stores are central. Multiple components use the store data.
3. There was a break in the Frontend due to some changes in the backend [that too just one GET API]. I tired to pipoint from which file is the issue occuring. This was not a good idea. Firstly, because tracing the frontend code was horrible [badly written code, too many files] and secondly, front end was a tedius system to deal with [Builds would take time. My system would get hanged on and off]. A better Idea was to Realize that only something had been changed within a GET API and to understand some key that was missing/not present in the response. 

## AWS cloud watch quirks
1. Sometimes the logs do not get echoed - we need to wait for a while since aws sends a task timmed out error later on



## Production Bugs:
   1. old summary is still getting generated

## Challenges faced while solving the production bug
1. Mistake - not interpreting the code correctly - this became misleading 
2. Deleting a lambda function without thinking about consequences.
3. A worry that the old staging (stg) lambda wasn't entirely deleted - this is beacuse logs were still not getting generated on production lambda - the root cause of this:
   1. another staging (stag) lambda was deployed
   2. besides deleting the staging (stag), the logs were still not available. Mistake: Here, I was not interpreting the logs of AWS correctly - In AWS, logs are grouped together. So, every time a submit a pdf, I expected a new log group to appear. Instead, new logs get appended to an existing log group.
   (a combination of 1. and 2. caused a lot of tediousness)
4. deleting the old staging (stg) lambda got me into a mess, since I thought it is not completely deleted and I had to recover it. This process was complicated, because certain cloudformation, aws buckets, and severless internal framework algorithms come into play.

